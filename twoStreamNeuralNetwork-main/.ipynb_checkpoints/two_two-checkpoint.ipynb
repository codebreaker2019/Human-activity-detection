{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcfn\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "#import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "#from google.colab.patches import cv2_imshow\n",
    "#from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2 as cv \n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"mcfn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joints_array= (11082, 72, 25, 3)\n",
      "labels= (11082, 100)\n",
      "featchers_array (11082, 7, 72)\n"
     ]
    }
   ],
   "source": [
    "### JOINTS FEATCHERS CODE #####\n",
    "\n",
    "\n",
    "path='E:/two stream final/'\n",
    "import numpy as np\n",
    "file1=open(path+\"jointsFile\",\"rb\")\n",
    "file2 = open(path+\"levelFile\", \"rb\")\n",
    "with open(path+'featcherFile', 'rb') as file3:\n",
    "    featchers_array=pickle.load(file3)\n",
    "    \n",
    "joints_array= np.load(file1)\n",
    "labels= np.load(file2)\n",
    "\n",
    "print(\"joints_array=\",joints_array.shape)\n",
    "print(\"labels=\",labels.shape)\n",
    "print(\"featchers_array\",featchers_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8311, 7, 72)\n",
      "(2771, 7, 72)\n",
      "(2771, 7, 72)\n"
     ]
    }
   ],
   "source": [
    "(trainFeatcherX, testFeatcherX, trainFeLabelY, testFeLabelY) = train_test_split(featchers_array,labels,\n",
    "\ttest_size=0.25 , random_state=42)\n",
    "print(trainFeatcherX.shape)\n",
    "print(testFeatcherX.shape)\n",
    "print(testFeatcherX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our LSTM network\n",
    "model1 = keras.Sequential()\n",
    "model1.add(keras.Input(shape=(7,72)))\n",
    "model1.add(LSTM(32))\n",
    "model1.add(Dense(8, activation=\"relu\"))\n",
    "model1.add(Dense(4, activation=\"relu\"))\n",
    "#model1.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"E:/two stream final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/two stream final/ALL_IMAGE.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e194e480e02a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mOptical_image_lables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'ALL_IMAGE.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0moptical_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/two stream final/ALL_IMAGE.data'"
     ]
    }
   ],
   "source": [
    "#### OPTICAL FLOW CODE#####\n",
    "with open(path+'label.data', 'rb') as l:\n",
    "    Optical_image_lables=pickle.load(l)\n",
    "    \n",
    "with open(path+'ALL_IMAGE.data', 'rb') as f:\n",
    "    optical_images=pickle.load(f)\n",
    "\n",
    "\n",
    "### ONE HOT ENCODING ####\n",
    "Optical_image_lables = np.asarray(Optical_image_lables)\n",
    "#lb = LabelBinarizer()\n",
    "#Optical_image_lables = lb.fit_transform(Optical_image_lables)\n",
    "\n",
    "print(\"optical_images=\",optical_images.shape)\n",
    "print(\"Optical_image_lables=\",Optical_image_lables.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9897,)\n"
     ]
    }
   ],
   "source": [
    "(trainImageX, testImageX, trainImageLabelY, tesImageLabeltY) = train_test_split(optical_images, Optical_image_lables,\n",
    "\ttest_size=0.25, stratify=Optical_image_lables, random_state=42)\n",
    "\n",
    "print(trainImageLabelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9897, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "trainImageLabelY = np_utils.to_categorical(trainImageLabelY)\n",
    "tesImageLabeltY = np_utils.to_categorical(tesImageLabeltY)\n",
    "print(trainImageLabelY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2771, 240, 320, 3)\n",
      "(8311, 240, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "trainImageX=trainImageX[:8311]\n",
    "trainImageLabelY=trainImageLabelY[:8311]\n",
    "testImageX=testImageX[:2771]\n",
    "tesImageLabeltY=tesImageLabeltY[:2771]\n",
    "print(testImageX.shape)\n",
    "print(trainImageX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our CNN network\n",
    "model2_in=Input(shape=(240, 320, 3))\n",
    "\n",
    "baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "     input_tensor=model2_in)\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "model2_out = baseModel.output\n",
    "model2_out = AveragePooling2D(pool_size=(7, 7))(model2_out)\n",
    "model2_out = Flatten(name=\"flatten\")(model2_out)\n",
    "model2_out = Dense(512, activation=\"relu\")(model2_out)\n",
    "model2_out = Dropout(0.5)(model2_out)\n",
    "#model2_out=Dense(1, activation=\"linear\")(model2_out)\n",
    "\n",
    "\n",
    "model2 = Model(inputs=model2_in, outputs=model2_out)\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model=Input(input_shape=(32, 32, 3))\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu',input_tensor=model))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(keras.layers.Dense(10))\n",
    "\n",
    "#model2 = Model(inputs=model2_in, outputs=model2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMBINED TWO MODELS ###\n",
    "combined = concatenate([model1.output, model2.output])\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(100, activation=\"softmax\")(z)\n",
    "model = Model(inputs=[model1.input, model2.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Epoch 1/300\n",
      "64/64 [==============================] - 1173s 18s/step - loss: 4.3335 - accuracy: 0.0274 - val_loss: 3.8534 - val_accuracy: 0.0693\n",
      "Epoch 2/300\n",
      "64/64 [==============================] - 1157s 18s/step - loss: 3.7928 - accuracy: 0.0608 - val_loss: 3.5970 - val_accuracy: 0.0808\n",
      "Epoch 3/300\n",
      "64/64 [==============================] - 1155s 18s/step - loss: 3.5843 - accuracy: 0.0770 - val_loss: 3.4278 - val_accuracy: 0.0913\n",
      "Epoch 4/300\n",
      "64/64 [==============================] - 1156s 18s/step - loss: 3.4283 - accuracy: 0.0876 - val_loss: 3.2863 - val_accuracy: 0.1267\n",
      "Epoch 5/300\n",
      "64/64 [==============================] - 1158s 18s/step - loss: 3.3150 - accuracy: 0.0945 - val_loss: 3.1967 - val_accuracy: 0.1140\n",
      "Epoch 6/300\n",
      "64/64 [==============================] - 1160s 18s/step - loss: 3.2391 - accuracy: 0.0974 - val_loss: 3.1379 - val_accuracy: 0.1501\n",
      "Epoch 7/300\n",
      "64/64 [==============================] - 1142s 18s/step - loss: 3.1433 - accuracy: 0.1186 - val_loss: 3.0885 - val_accuracy: 0.1519\n",
      "Epoch 8/300\n",
      "64/64 [==============================] - 1136s 18s/step - loss: 3.1046 - accuracy: 0.1252 - val_loss: 3.0313 - val_accuracy: 0.1440\n",
      "Epoch 9/300\n",
      "64/64 [==============================] - 1143s 18s/step - loss: 3.0548 - accuracy: 0.1321 - val_loss: 2.9688 - val_accuracy: 0.1508\n",
      "Epoch 10/300\n",
      "64/64 [==============================] - 1160s 18s/step - loss: 3.0331 - accuracy: 0.1355 - val_loss: 2.9361 - val_accuracy: 0.1494\n",
      "Epoch 11/300\n",
      "64/64 [==============================] - 1139s 18s/step - loss: 2.9557 - accuracy: 0.1410 - val_loss: 2.8861 - val_accuracy: 0.1822\n",
      "Epoch 12/300\n",
      "64/64 [==============================] - 1135s 18s/step - loss: 2.9116 - accuracy: 0.1465 - val_loss: 2.8367 - val_accuracy: 0.1931\n",
      "Epoch 13/300\n",
      "64/64 [==============================] - 1159s 18s/step - loss: 2.8795 - accuracy: 0.1608 - val_loss: 2.8034 - val_accuracy: 0.2223\n",
      "Epoch 14/300\n",
      "64/64 [==============================] - 1155s 18s/step - loss: 2.8602 - accuracy: 0.1678 - val_loss: 2.7848 - val_accuracy: 0.2133\n",
      "Epoch 15/300\n",
      "64/64 [==============================] - 1157s 18s/step - loss: 2.8257 - accuracy: 0.1678 - val_loss: 2.7512 - val_accuracy: 0.2129\n",
      "Epoch 16/300\n",
      "64/64 [==============================] - 1157s 18s/step - loss: 2.7894 - accuracy: 0.1718 - val_loss: 2.7003 - val_accuracy: 0.1808\n",
      "Epoch 17/300\n",
      "64/64 [==============================] - 1168s 18s/step - loss: 2.7197 - accuracy: 0.1939 - val_loss: 2.6727 - val_accuracy: 0.1923\n",
      "Epoch 18/300\n",
      "64/64 [==============================] - 1160s 18s/step - loss: 2.7259 - accuracy: 0.1986 - val_loss: 2.6580 - val_accuracy: 0.2216\n",
      "Epoch 19/300\n",
      "64/64 [==============================] - 1140s 18s/step - loss: 2.6795 - accuracy: 0.2057 - val_loss: 2.6289 - val_accuracy: 0.2266\n",
      "Epoch 20/300\n",
      "64/64 [==============================] - 1140s 18s/step - loss: 2.6446 - accuracy: 0.2181 - val_loss: 2.5681 - val_accuracy: 0.2512\n",
      "Epoch 21/300\n",
      "64/64 [==============================] - 1144s 18s/step - loss: 2.6317 - accuracy: 0.2278 - val_loss: 2.5606 - val_accuracy: 0.2317\n",
      "Epoch 22/300\n",
      "64/64 [==============================] - 1137s 18s/step - loss: 2.6066 - accuracy: 0.2373 - val_loss: 2.5486 - val_accuracy: 0.2367\n",
      "Epoch 23/300\n",
      "64/64 [==============================] - 1139s 18s/step - loss: 2.5722 - accuracy: 0.2431 - val_loss: 2.5474 - val_accuracy: 0.2912\n",
      "Epoch 24/300\n",
      "64/64 [==============================] - 1138s 18s/step - loss: 2.5873 - accuracy: 0.2479 - val_loss: 2.5154 - val_accuracy: 0.3190\n",
      "Epoch 25/300\n",
      "64/64 [==============================] - 1141s 18s/step - loss: 2.5487 - accuracy: 0.2543 - val_loss: 2.4783 - val_accuracy: 0.2891\n",
      "Epoch 26/300\n",
      "64/64 [==============================] - 1150s 18s/step - loss: 2.5168 - accuracy: 0.2678 - val_loss: 2.4496 - val_accuracy: 0.3259\n",
      "Epoch 27/300\n",
      "64/64 [==============================] - 1148s 18s/step - loss: 2.5132 - accuracy: 0.2742 - val_loss: 2.4286 - val_accuracy: 0.2981\n",
      "Epoch 28/300\n",
      "64/64 [==============================] - 1166s 18s/step - loss: 2.4803 - accuracy: 0.2890 - val_loss: 2.4191 - val_accuracy: 0.3183\n",
      "Epoch 29/300\n",
      "64/64 [==============================] - 1170s 18s/step - loss: 2.4821 - accuracy: 0.2854 - val_loss: 2.4096 - val_accuracy: 0.3093\n",
      "Epoch 30/300\n",
      "64/64 [==============================] - 1192s 19s/step - loss: 2.4341 - accuracy: 0.3016 - val_loss: 2.3632 - val_accuracy: 0.3183\n",
      "Epoch 31/300\n",
      "64/64 [==============================] - 1266s 20s/step - loss: 2.4134 - accuracy: 0.2960 - val_loss: 2.3676 - val_accuracy: 0.3003\n",
      "Epoch 32/300\n",
      "64/64 [==============================] - ETA: 0s - loss: 2.4163 - accuracy: 0.3048 "
     ]
    }
   ],
   "source": [
    "opt = SGD(lr=1e-4, momentum=0.9, decay=1e-4 / 10)\n",
    "model.compile(loss=\"CategoricalCrossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    [trainFeatcherX, trainImageX],trainImageLabelY,\n",
    "    validation_data=([testFeatcherX, testImageX], tesImageLabeltY),\n",
    "    epochs=300, batch_size=130)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting ...\")\n",
    "preds = model.predict([testFeatcherX, testImageX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
